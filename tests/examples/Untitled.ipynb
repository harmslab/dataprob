{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bf33d530-f33a-4cfe-9e15-a42e22838671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p_/hcs03cdj48nbsvw72d92tr040000gn/T/ipykernel_52576/950956994.py:8: UserWarning: \n",
      "\n",
      "blah\n",
      "\n",
      "The fitter threw the following:\n",
      "  '1', 2 times.\n",
      "  '2', 1 times.\n",
      "\n",
      "  warnings.warn(w)\n"
     ]
    }
   ],
   "source": [
    "w = \"\\n\\nblah\\n\\n\"\n",
    "prob_types, prob_counts = np.unique([\"1\",\"2\",\"1\"],return_counts=True)\n",
    "w += \"The fitter threw the following:\\n\"\n",
    "for i in range(len(prob_types)):\n",
    "    w += f\"  '{prob_types[i]}', {prob_counts[i]} times.\\n\"\n",
    "\n",
    "import warnings\n",
    "warnings.warn(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c3081d4e-bef4-4443-847a-9503f9b5c0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function least_squares in module scipy.optimize._lsq.least_squares:\n",
      "\n",
      "least_squares(fun, x0, jac='2-point', bounds=(-inf, inf), method='trf', ftol=1e-08, xtol=1e-08, gtol=1e-08, x_scale=1.0, loss='linear', f_scale=1.0, diff_step=None, tr_solver=None, tr_options={}, jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={})\n",
      "    Solve a nonlinear least-squares problem with bounds on the variables.\n",
      "\n",
      "    Given the residuals f(x) (an m-D real function of n real\n",
      "    variables) and the loss function rho(s) (a scalar function), `least_squares`\n",
      "    finds a local minimum of the cost function F(x)::\n",
      "\n",
      "        minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\n",
      "        subject to lb <= x <= ub\n",
      "\n",
      "    The purpose of the loss function rho(s) is to reduce the influence of\n",
      "    outliers on the solution.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        Function which computes the vector of residuals, with the signature\n",
      "        ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\n",
      "        respect to its first argument. The argument ``x`` passed to this\n",
      "        function is an ndarray of shape (n,) (never a scalar, even for n=1).\n",
      "        It must allocate and return a 1-D array_like of shape (m,) or a scalar.\n",
      "        If the argument ``x`` is complex or the function ``fun`` returns\n",
      "        complex residuals, it must be wrapped in a real function of real\n",
      "        arguments, as shown at the end of the Examples section.\n",
      "    x0 : array_like with shape (n,) or float\n",
      "        Initial guess on independent variables. If float, it will be treated\n",
      "        as a 1-D array with one element. When `method` is 'trf', the initial\n",
      "        guess might be slightly adjusted to lie sufficiently within the given\n",
      "        `bounds`.\n",
      "    jac : {'2-point', '3-point', 'cs', callable}, optional\n",
      "        Method of computing the Jacobian matrix (an m-by-n matrix, where\n",
      "        element (i, j) is the partial derivative of f[i] with respect to\n",
      "        x[j]). The keywords select a finite difference scheme for numerical\n",
      "        estimation. The scheme '3-point' is more accurate, but requires\n",
      "        twice as many operations as '2-point' (default). The scheme 'cs'\n",
      "        uses complex steps, and while potentially the most accurate, it is\n",
      "        applicable only when `fun` correctly handles complex inputs and\n",
      "        can be analytically continued to the complex plane. Method 'lm'\n",
      "        always uses the '2-point' scheme. If callable, it is used as\n",
      "        ``jac(x, *args, **kwargs)`` and should return a good approximation\n",
      "        (or the exact value) for the Jacobian as an array_like (np.atleast_2d\n",
      "        is applied), a sparse matrix (csr_matrix preferred for performance) or\n",
      "        a `scipy.sparse.linalg.LinearOperator`.\n",
      "    bounds : 2-tuple of array_like or `Bounds`, optional\n",
      "        There are two ways to specify bounds:\n",
      "\n",
      "            1. Instance of `Bounds` class\n",
      "            2. Lower and upper bounds on independent variables. Defaults to no\n",
      "               bounds. Each array must match the size of `x0` or be a scalar,\n",
      "               in the latter case a bound will be the same for all variables.\n",
      "               Use ``np.inf`` with an appropriate sign to disable bounds on all\n",
      "               or some variables.\n",
      "    method : {'trf', 'dogbox', 'lm'}, optional\n",
      "        Algorithm to perform minimization.\n",
      "\n",
      "            * 'trf' : Trust Region Reflective algorithm, particularly suitable\n",
      "              for large sparse problems with bounds. Generally robust method.\n",
      "            * 'dogbox' : dogleg algorithm with rectangular trust regions,\n",
      "              typical use case is small problems with bounds. Not recommended\n",
      "              for problems with rank-deficient Jacobian.\n",
      "            * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\n",
      "              Doesn't handle bounds and sparse Jacobians. Usually the most\n",
      "              efficient method for small unconstrained problems.\n",
      "\n",
      "        Default is 'trf'. See Notes for more information.\n",
      "    ftol : float or None, optional\n",
      "        Tolerance for termination by the change of the cost function. Default\n",
      "        is 1e-8. The optimization process is stopped when ``dF < ftol * F``,\n",
      "        and there was an adequate agreement between a local quadratic model and\n",
      "        the true model in the last step.\n",
      "\n",
      "        If None and 'method' is not 'lm', the termination by this condition is\n",
      "        disabled. If 'method' is 'lm', this tolerance must be higher than\n",
      "        machine epsilon.\n",
      "    xtol : float or None, optional\n",
      "        Tolerance for termination by the change of the independent variables.\n",
      "        Default is 1e-8. The exact condition depends on the `method` used:\n",
      "\n",
      "            * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``.\n",
      "            * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\n",
      "              a trust-region radius and ``xs`` is the value of ``x``\n",
      "              scaled according to `x_scale` parameter (see below).\n",
      "\n",
      "        If None and 'method' is not 'lm', the termination by this condition is\n",
      "        disabled. If 'method' is 'lm', this tolerance must be higher than\n",
      "        machine epsilon.\n",
      "    gtol : float or None, optional\n",
      "        Tolerance for termination by the norm of the gradient. Default is 1e-8.\n",
      "        The exact condition depends on a `method` used:\n",
      "\n",
      "            * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\n",
      "              ``g_scaled`` is the value of the gradient scaled to account for\n",
      "              the presence of the bounds [STIR]_.\n",
      "            * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\n",
      "              ``g_free`` is the gradient with respect to the variables which\n",
      "              are not in the optimal state on the boundary.\n",
      "            * For 'lm' : the maximum absolute value of the cosine of angles\n",
      "              between columns of the Jacobian and the residual vector is less\n",
      "              than `gtol`, or the residual vector is zero.\n",
      "\n",
      "        If None and 'method' is not 'lm', the termination by this condition is\n",
      "        disabled. If 'method' is 'lm', this tolerance must be higher than\n",
      "        machine epsilon.\n",
      "    x_scale : array_like or 'jac', optional\n",
      "        Characteristic scale of each variable. Setting `x_scale` is equivalent\n",
      "        to reformulating the problem in scaled variables ``xs = x / x_scale``.\n",
      "        An alternative view is that the size of a trust region along jth\n",
      "        dimension is proportional to ``x_scale[j]``. Improved convergence may\n",
      "        be achieved by setting `x_scale` such that a step of a given size\n",
      "        along any of the scaled variables has a similar effect on the cost\n",
      "        function. If set to 'jac', the scale is iteratively updated using the\n",
      "        inverse norms of the columns of the Jacobian matrix (as described in\n",
      "        [JJMore]_).\n",
      "    loss : str or callable, optional\n",
      "        Determines the loss function. The following keyword values are allowed:\n",
      "\n",
      "            * 'linear' (default) : ``rho(z) = z``. Gives a standard\n",
      "              least-squares problem.\n",
      "            * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\n",
      "              approximation of l1 (absolute value) loss. Usually a good\n",
      "              choice for robust least squares.\n",
      "            * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5 - 1``. Works\n",
      "              similarly to 'soft_l1'.\n",
      "            * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\n",
      "              influence, but may cause difficulties in optimization process.\n",
      "            * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\n",
      "              a single residual, has properties similar to 'cauchy'.\n",
      "\n",
      "        If callable, it must take a 1-D ndarray ``z=f**2`` and return an\n",
      "        array_like with shape (3, m) where row 0 contains function values,\n",
      "        row 1 contains first derivatives and row 2 contains second\n",
      "        derivatives. Method 'lm' supports only 'linear' loss.\n",
      "    f_scale : float, optional\n",
      "        Value of soft margin between inlier and outlier residuals, default\n",
      "        is 1.0. The loss function is evaluated as follows\n",
      "        ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\n",
      "        and ``rho`` is determined by `loss` parameter. This parameter has\n",
      "        no effect with ``loss='linear'``, but for other `loss` values it is\n",
      "        of crucial importance.\n",
      "    max_nfev : None or int, optional\n",
      "        Maximum number of function evaluations before the termination.\n",
      "        If None (default), the value is chosen automatically:\n",
      "\n",
      "            * For 'trf' and 'dogbox' : 100 * n.\n",
      "            * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)\n",
      "              otherwise (because 'lm' counts function calls in Jacobian\n",
      "              estimation).\n",
      "\n",
      "    diff_step : None or array_like, optional\n",
      "        Determines the relative step size for the finite difference\n",
      "        approximation of the Jacobian. The actual step is computed as\n",
      "        ``x * diff_step``. If None (default), then `diff_step` is taken to be\n",
      "        a conventional \"optimal\" power of machine epsilon for the finite\n",
      "        difference scheme used [NR]_.\n",
      "    tr_solver : {None, 'exact', 'lsmr'}, optional\n",
      "        Method for solving trust-region subproblems, relevant only for 'trf'\n",
      "        and 'dogbox' methods.\n",
      "\n",
      "            * 'exact' is suitable for not very large problems with dense\n",
      "              Jacobian matrices. The computational complexity per iteration is\n",
      "              comparable to a singular value decomposition of the Jacobian\n",
      "              matrix.\n",
      "            * 'lsmr' is suitable for problems with sparse and large Jacobian\n",
      "              matrices. It uses the iterative procedure\n",
      "              `scipy.sparse.linalg.lsmr` for finding a solution of a linear\n",
      "              least-squares problem and only requires matrix-vector product\n",
      "              evaluations.\n",
      "\n",
      "        If None (default), the solver is chosen based on the type of Jacobian\n",
      "        returned on the first iteration.\n",
      "    tr_options : dict, optional\n",
      "        Keyword options passed to trust-region solver.\n",
      "\n",
      "            * ``tr_solver='exact'``: `tr_options` are ignored.\n",
      "            * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\n",
      "              Additionally,  ``method='trf'`` supports  'regularize' option\n",
      "              (bool, default is True), which adds a regularization term to the\n",
      "              normal equation, which improves convergence if the Jacobian is\n",
      "              rank-deficient [Byrd]_ (eq. 3.4).\n",
      "\n",
      "    jac_sparsity : {None, array_like, sparse matrix}, optional\n",
      "        Defines the sparsity structure of the Jacobian matrix for finite\n",
      "        difference estimation, its shape must be (m, n). If the Jacobian has\n",
      "        only few non-zero elements in *each* row, providing the sparsity\n",
      "        structure will greatly speed up the computations [Curtis]_. A zero\n",
      "        entry means that a corresponding element in the Jacobian is identically\n",
      "        zero. If provided, forces the use of 'lsmr' trust-region solver.\n",
      "        If None (default), then dense differencing will be used. Has no effect\n",
      "        for 'lm' method.\n",
      "    verbose : {0, 1, 2}, optional\n",
      "        Level of algorithm's verbosity:\n",
      "\n",
      "            * 0 (default) : work silently.\n",
      "            * 1 : display a termination report.\n",
      "            * 2 : display progress during iterations (not supported by 'lm'\n",
      "              method).\n",
      "\n",
      "    args, kwargs : tuple and dict, optional\n",
      "        Additional arguments passed to `fun` and `jac`. Both empty by default.\n",
      "        The calling signature is ``fun(x, *args, **kwargs)`` and the same for\n",
      "        `jac`.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    result : OptimizeResult\n",
      "        `OptimizeResult` with the following fields defined:\n",
      "\n",
      "            x : ndarray, shape (n,)\n",
      "                Solution found.\n",
      "            cost : float\n",
      "                Value of the cost function at the solution.\n",
      "            fun : ndarray, shape (m,)\n",
      "                Vector of residuals at the solution.\n",
      "            jac : ndarray, sparse matrix or LinearOperator, shape (m, n)\n",
      "                Modified Jacobian matrix at the solution, in the sense that J^T J\n",
      "                is a Gauss-Newton approximation of the Hessian of the cost function.\n",
      "                The type is the same as the one used by the algorithm.\n",
      "            grad : ndarray, shape (m,)\n",
      "                Gradient of the cost function at the solution.\n",
      "            optimality : float\n",
      "                First-order optimality measure. In unconstrained problems, it is\n",
      "                always the uniform norm of the gradient. In constrained problems,\n",
      "                it is the quantity which was compared with `gtol` during iterations.\n",
      "            active_mask : ndarray of int, shape (n,)\n",
      "                Each component shows whether a corresponding constraint is active\n",
      "                (that is, whether a variable is at the bound):\n",
      "\n",
      "                    *  0 : a constraint is not active.\n",
      "                    * -1 : a lower bound is active.\n",
      "                    *  1 : an upper bound is active.\n",
      "\n",
      "                Might be somewhat arbitrary for 'trf' method as it generates a\n",
      "                sequence of strictly feasible iterates and `active_mask` is\n",
      "                determined within a tolerance threshold.\n",
      "            nfev : int\n",
      "                Number of function evaluations done. Methods 'trf' and 'dogbox' do\n",
      "                not count function calls for numerical Jacobian approximation, as\n",
      "                opposed to 'lm' method.\n",
      "            njev : int or None\n",
      "                Number of Jacobian evaluations done. If numerical Jacobian\n",
      "                approximation is used in 'lm' method, it is set to None.\n",
      "            status : int\n",
      "                The reason for algorithm termination:\n",
      "\n",
      "                    * -1 : improper input parameters status returned from MINPACK.\n",
      "                    *  0 : the maximum number of function evaluations is exceeded.\n",
      "                    *  1 : `gtol` termination condition is satisfied.\n",
      "                    *  2 : `ftol` termination condition is satisfied.\n",
      "                    *  3 : `xtol` termination condition is satisfied.\n",
      "                    *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\n",
      "\n",
      "            message : str\n",
      "                Verbal description of the termination reason.\n",
      "            success : bool\n",
      "                True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    leastsq : A legacy wrapper for the MINPACK implementation of the\n",
      "              Levenberg-Marquadt algorithm.\n",
      "    curve_fit : Least-squares minimization applied to a curve-fitting problem.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares\n",
      "    algorithms implemented in MINPACK (lmder, lmdif). It runs the\n",
      "    Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\n",
      "    The implementation is based on paper [JJMore]_, it is very robust and\n",
      "    efficient with a lot of smart tricks. It should be your first choice\n",
      "    for unconstrained problems. Note that it doesn't support bounds. Also,\n",
      "    it doesn't work when m < n.\n",
      "\n",
      "    Method 'trf' (Trust Region Reflective) is motivated by the process of\n",
      "    solving a system of equations, which constitute the first-order optimality\n",
      "    condition for a bound-constrained minimization problem as formulated in\n",
      "    [STIR]_. The algorithm iteratively solves trust-region subproblems\n",
      "    augmented by a special diagonal quadratic term and with trust-region shape\n",
      "    determined by the distance from the bounds and the direction of the\n",
      "    gradient. This enhancements help to avoid making steps directly into bounds\n",
      "    and efficiently explore the whole space of variables. To further improve\n",
      "    convergence, the algorithm considers search directions reflected from the\n",
      "    bounds. To obey theoretical requirements, the algorithm keeps iterates\n",
      "    strictly feasible. With dense Jacobians trust-region subproblems are\n",
      "    solved by an exact method very similar to the one described in [JJMore]_\n",
      "    (and implemented in MINPACK). The difference from the MINPACK\n",
      "    implementation is that a singular value decomposition of a Jacobian\n",
      "    matrix is done once per iteration, instead of a QR decomposition and series\n",
      "    of Givens rotation eliminations. For large sparse Jacobians a 2-D subspace\n",
      "    approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\n",
      "    The subspace is spanned by a scaled gradient and an approximate\n",
      "    Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\n",
      "    constraints are imposed the algorithm is very similar to MINPACK and has\n",
      "    generally comparable performance. The algorithm works quite robust in\n",
      "    unbounded and bounded problems, thus it is chosen as a default algorithm.\n",
      "\n",
      "    Method 'dogbox' operates in a trust-region framework, but considers\n",
      "    rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\n",
      "    The intersection of a current trust region and initial bounds is again\n",
      "    rectangular, so on each iteration a quadratic minimization problem subject\n",
      "    to bound constraints is solved approximately by Powell's dogleg method\n",
      "    [NumOpt]_. The required Gauss-Newton step can be computed exactly for\n",
      "    dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\n",
      "    sparse Jacobians. The algorithm is likely to exhibit slow convergence when\n",
      "    the rank of Jacobian is less than the number of variables. The algorithm\n",
      "    often outperforms 'trf' in bounded problems with a small number of\n",
      "    variables.\n",
      "\n",
      "    Robust loss functions are implemented as described in [BA]_. The idea\n",
      "    is to modify a residual vector and a Jacobian matrix on each iteration\n",
      "    such that computed gradient and Gauss-Newton Hessian approximation match\n",
      "    the true gradient and Hessian approximation of the cost function. Then\n",
      "    the algorithm proceeds in a normal way, i.e., robust loss functions are\n",
      "    implemented as a simple wrapper over standard least-squares algorithms.\n",
      "\n",
      "    .. versionadded:: 0.17.0\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "              and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "              Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "              Vol. 21, Number 1, pp 1-23, 1999.\n",
      "    .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\n",
      "            Computing. 3rd edition\", Sec. 5.7.\n",
      "    .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\n",
      "              solution of the trust region problem by minimization over\n",
      "              two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\n",
      "              1988.\n",
      "    .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\n",
      "                sparse Jacobian matrices\", Journal of the Institute of\n",
      "                Mathematics and its Applications, 13, pp. 117-120, 1974.\n",
      "    .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\n",
      "                and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\n",
      "                Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\n",
      "    .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\n",
      "                Dogleg Approach for Unconstrained and Bound Constrained\n",
      "                Nonlinear Optimization\", WSEAS International Conference on\n",
      "                Applied Mathematics, Corfu, Greece, 2004.\n",
      "    .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\n",
      "                2nd edition\", Chapter 4.\n",
      "    .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\n",
      "            Proceedings of the International Workshop on Vision Algorithms:\n",
      "            Theory and Practice, pp. 298-372, 1999.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    In this example we find a minimum of the Rosenbrock function without bounds\n",
      "    on independent variables.\n",
      "\n",
      "    >>> import numpy as np\n",
      "    >>> def fun_rosenbrock(x):\n",
      "    ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\n",
      "\n",
      "    Notice that we only provide the vector of the residuals. The algorithm\n",
      "    constructs the cost function as a sum of squares of the residuals, which\n",
      "    gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\n",
      "\n",
      "    >>> from scipy.optimize import least_squares\n",
      "    >>> x0_rosenbrock = np.array([2, 2])\n",
      "    >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\n",
      "    >>> res_1.x\n",
      "    array([ 1.,  1.])\n",
      "    >>> res_1.cost\n",
      "    9.8669242910846867e-30\n",
      "    >>> res_1.optimality\n",
      "    8.8928864934219529e-14\n",
      "\n",
      "    We now constrain the variables, in such a way that the previous solution\n",
      "    becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\n",
      "    ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\n",
      "    to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\n",
      "\n",
      "    We also provide the analytic Jacobian:\n",
      "\n",
      "    >>> def jac_rosenbrock(x):\n",
      "    ...     return np.array([\n",
      "    ...         [-20 * x[0], 10],\n",
      "    ...         [-1, 0]])\n",
      "\n",
      "    Putting this all together, we see that the new solution lies on the bound:\n",
      "\n",
      "    >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\n",
      "    ...                       bounds=([-np.inf, 1.5], np.inf))\n",
      "    >>> res_2.x\n",
      "    array([ 1.22437075,  1.5       ])\n",
      "    >>> res_2.cost\n",
      "    0.025213093946805685\n",
      "    >>> res_2.optimality\n",
      "    1.5885401433157753e-07\n",
      "\n",
      "    Now we solve a system of equations (i.e., the cost function should be zero\n",
      "    at a minimum) for a Broyden tridiagonal vector-valued function of 100000\n",
      "    variables:\n",
      "\n",
      "    >>> def fun_broyden(x):\n",
      "    ...     f = (3 - x) * x + 1\n",
      "    ...     f[1:] -= x[:-1]\n",
      "    ...     f[:-1] -= 2 * x[1:]\n",
      "    ...     return f\n",
      "\n",
      "    The corresponding Jacobian matrix is sparse. We tell the algorithm to\n",
      "    estimate it by finite differences and provide the sparsity structure of\n",
      "    Jacobian to significantly speed up this process.\n",
      "\n",
      "    >>> from scipy.sparse import lil_matrix\n",
      "    >>> def sparsity_broyden(n):\n",
      "    ...     sparsity = lil_matrix((n, n), dtype=int)\n",
      "    ...     i = np.arange(n)\n",
      "    ...     sparsity[i, i] = 1\n",
      "    ...     i = np.arange(1, n)\n",
      "    ...     sparsity[i, i - 1] = 1\n",
      "    ...     i = np.arange(n - 1)\n",
      "    ...     sparsity[i, i + 1] = 1\n",
      "    ...     return sparsity\n",
      "    ...\n",
      "    >>> n = 100000\n",
      "    >>> x0_broyden = -np.ones(n)\n",
      "    ...\n",
      "    >>> res_3 = least_squares(fun_broyden, x0_broyden,\n",
      "    ...                       jac_sparsity=sparsity_broyden(n))\n",
      "    >>> res_3.cost\n",
      "    4.5687069299604613e-23\n",
      "    >>> res_3.optimality\n",
      "    1.1650454296851518e-11\n",
      "\n",
      "    Let's also solve a curve fitting problem using robust loss function to\n",
      "    take care of outliers in the data. Define the model function as\n",
      "    ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\n",
      "    observation and a, b, c are parameters to estimate.\n",
      "\n",
      "    First, define the function which generates the data with noise and\n",
      "    outliers, define the model parameters, and generate data:\n",
      "\n",
      "    >>> from numpy.random import default_rng\n",
      "    >>> rng = default_rng()\n",
      "    >>> def gen_data(t, a, b, c, noise=0., n_outliers=0, seed=None):\n",
      "    ...     rng = default_rng(seed)\n",
      "    ...\n",
      "    ...     y = a + b * np.exp(t * c)\n",
      "    ...\n",
      "    ...     error = noise * rng.standard_normal(t.size)\n",
      "    ...     outliers = rng.integers(0, t.size, n_outliers)\n",
      "    ...     error[outliers] *= 10\n",
      "    ...\n",
      "    ...     return y + error\n",
      "    ...\n",
      "    >>> a = 0.5\n",
      "    >>> b = 2.0\n",
      "    >>> c = -1\n",
      "    >>> t_min = 0\n",
      "    >>> t_max = 10\n",
      "    >>> n_points = 15\n",
      "    ...\n",
      "    >>> t_train = np.linspace(t_min, t_max, n_points)\n",
      "    >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\n",
      "\n",
      "    Define function for computing residuals and initial estimate of\n",
      "    parameters.\n",
      "\n",
      "    >>> def fun(x, t, y):\n",
      "    ...     return x[0] + x[1] * np.exp(x[2] * t) - y\n",
      "    ...\n",
      "    >>> x0 = np.array([1.0, 1.0, 0.0])\n",
      "\n",
      "    Compute a standard least-squares solution:\n",
      "\n",
      "    >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\n",
      "\n",
      "    Now compute two solutions with two different robust loss functions. The\n",
      "    parameter `f_scale` is set to 0.1, meaning that inlier residuals should\n",
      "    not significantly exceed 0.1 (the noise level used).\n",
      "\n",
      "    >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\n",
      "    ...                             args=(t_train, y_train))\n",
      "    >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\n",
      "    ...                         args=(t_train, y_train))\n",
      "\n",
      "    And, finally, plot all the curves. We see that by selecting an appropriate\n",
      "    `loss`  we can get estimates close to optimal even in the presence of\n",
      "    strong outliers. But keep in mind that generally it is recommended to try\n",
      "    'soft_l1' or 'huber' losses first (if at all necessary) as the other two\n",
      "    options may cause difficulties in optimization process.\n",
      "\n",
      "    >>> t_test = np.linspace(t_min, t_max, n_points * 10)\n",
      "    >>> y_true = gen_data(t_test, a, b, c)\n",
      "    >>> y_lsq = gen_data(t_test, *res_lsq.x)\n",
      "    >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
      "    >>> y_log = gen_data(t_test, *res_log.x)\n",
      "    ...\n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> plt.plot(t_train, y_train, 'o')\n",
      "    >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\n",
      "    >>> plt.plot(t_test, y_lsq, label='linear loss')\n",
      "    >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\n",
      "    >>> plt.plot(t_test, y_log, label='cauchy loss')\n",
      "    >>> plt.xlabel(\"t\")\n",
      "    >>> plt.ylabel(\"y\")\n",
      "    >>> plt.legend()\n",
      "    >>> plt.show()\n",
      "\n",
      "    In the next example, we show how complex-valued residual functions of\n",
      "    complex variables can be optimized with ``least_squares()``. Consider the\n",
      "    following function:\n",
      "\n",
      "    >>> def f(z):\n",
      "    ...     return z - (0.5 + 0.5j)\n",
      "\n",
      "    We wrap it into a function of real variables that returns real residuals\n",
      "    by simply handling the real and imaginary parts as independent variables:\n",
      "\n",
      "    >>> def f_wrap(x):\n",
      "    ...     fx = f(x[0] + 1j*x[1])\n",
      "    ...     return np.array([fx.real, fx.imag])\n",
      "\n",
      "    Thus, instead of the original m-D complex function of n complex\n",
      "    variables we optimize a 2m-D real function of 2n real variables:\n",
      "\n",
      "    >>> from scipy.optimize import least_squares\n",
      "    >>> res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))\n",
      "    >>> z = res_wrapped.x[0] + res_wrapped.x[1]*1j\n",
      "    >>> z\n",
      "    (0.49999999999925893+0.49999999999925893j)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(scipy.optimize.least_squares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4ddc68b9-e7e2-4731-b457-d78996da5e6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43me\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a3778aab-f95e-4dc1-ae02-c1ebddd01cf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Residuals are not finite in the initial point.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[167], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myo\u001b[39m(a): \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleast_squares\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/scipy/optimize/_lsq/least_squares.py:839\u001b[0m, in \u001b[0;36mleast_squares\u001b[0;34m(fun, x0, jac, bounds, method, ftol, xtol, gtol, x_scale, loss, f_scale, diff_step, tr_solver, tr_options, jac_sparsity, max_nfev, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` must return at most 1-d array_like. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    836\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf0.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf0\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39misfinite(f0)):\n\u001b[0;32m--> 839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResiduals are not finite in the initial point.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    841\u001b[0m n \u001b[38;5;241m=\u001b[39m x0\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m    842\u001b[0m m \u001b[38;5;241m=\u001b[39m f0\u001b[38;5;241m.\u001b[39msize\n",
      "\u001b[0;31mValueError\u001b[0m: Residuals are not finite in the initial point."
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "def yo(a): return np.nan*np.ones(10)\n",
    "\n",
    "scipy.optimize.least_squares(fun=yo,\n",
    "                             x0=np.ones(2,dtype=float),\n",
    "                             bounds=np.array([[-np.inf,-np.inf],[np.inf,np.inf]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "816f33e1-6223-4ba5-b862-f1ded5009767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import emcee\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed142691-9073-48fe-b3be-0c158fc4d5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_prob(param): return -param[0]*param[1]\n",
    "num_steps = 10\n",
    "num_walkers = 10\n",
    "ndim = 2\n",
    "pos = [np.random.normal(loc=0,scale=1,size=2)\n",
    "       for _ in range(num_walkers)]\n",
    "\n",
    "es = emcee.EnsembleSampler(num_walkers,\n",
    "                           ndim,\n",
    "                           ln_prob)\n",
    "yo = es.run_mcmc(pos,num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fec6475a-7dcf-4da9-8bb9-08ed012f08c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EnsembleSampler in module emcee.ensemble:\n",
      "\n",
      "class EnsembleSampler(builtins.object)\n",
      " |  EnsembleSampler(nwalkers, ndim, log_prob_fn, pool=None, moves=None, args=None, kwargs=None, backend=None, vectorize=False, blobs_dtype=None, parameter_names: Union[Dict[str, int], List[str], NoneType] = None, a=None, postargs=None, threads=None, live_dangerously=None, runtime_sortingfn=None)\n",
      " |\n",
      " |  An ensemble MCMC sampler\n",
      " |\n",
      " |  If you are upgrading from an earlier version of emcee, you might notice\n",
      " |  that some arguments are now deprecated. The parameters that control the\n",
      " |  proposals have been moved to the :ref:`moves-user` interface (``a`` and\n",
      " |  ``live_dangerously``), and the parameters related to parallelization can\n",
      " |  now be controlled via the ``pool`` argument (:ref:`parallel`).\n",
      " |\n",
      " |  Args:\n",
      " |      nwalkers (int): The number of walkers in the ensemble.\n",
      " |      ndim (int): Number of dimensions in the parameter space.\n",
      " |      log_prob_fn (callable): A function that takes a vector in the\n",
      " |          parameter space as input and returns the natural logarithm of the\n",
      " |          posterior probability (up to an additive constant) for that\n",
      " |          position.\n",
      " |      moves (Optional): This can be a single move object, a list of moves,\n",
      " |          or a \"weighted\" list of the form ``[(emcee.moves.StretchMove(),\n",
      " |          0.1), ...]``. When running, the sampler will randomly select a\n",
      " |          move from this list (optionally with weights) for each proposal.\n",
      " |          (default: :class:`StretchMove`)\n",
      " |      args (Optional): A list of extra positional arguments for\n",
      " |          ``log_prob_fn``. ``log_prob_fn`` will be called with the sequence\n",
      " |          ``log_prob_fn(p, *args, **kwargs)``.\n",
      " |      kwargs (Optional): A dict of extra keyword arguments for\n",
      " |          ``log_prob_fn``. ``log_prob_fn`` will be called with the sequence\n",
      " |          ``log_prob_fn(p, *args, **kwargs)``.\n",
      " |      pool (Optional): An object with a ``map`` method that follows the same\n",
      " |          calling sequence as the built-in ``map`` function. This is\n",
      " |          generally used to compute the log-probabilities for the ensemble\n",
      " |          in parallel.\n",
      " |      backend (Optional): Either a :class:`backends.Backend` or a subclass\n",
      " |          (like :class:`backends.HDFBackend`) that is used to store and\n",
      " |          serialize the state of the chain. By default, the chain is stored\n",
      " |          as a set of numpy arrays in memory, but new backends can be\n",
      " |          written to support other mediums.\n",
      " |      vectorize (Optional[bool]): If ``True``, ``log_prob_fn`` is expected\n",
      " |          to accept a list of position vectors instead of just one. Note\n",
      " |          that ``pool`` will be ignored if this is ``True``.\n",
      " |          (default: ``False``)\n",
      " |      parameter_names (Optional[Union[List[str], Dict[str, List[int]]]]):\n",
      " |          names of individual parameters or groups of parameters. If\n",
      " |          specified, the ``log_prob_fn`` will recieve a dictionary of\n",
      " |          parameters, rather than a ``np.ndarray``.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __init__(self, nwalkers, ndim, log_prob_fn, pool=None, moves=None, args=None, kwargs=None, backend=None, vectorize=False, blobs_dtype=None, parameter_names: Union[Dict[str, int], List[str], NoneType] = None, a=None, postargs=None, threads=None, live_dangerously=None, runtime_sortingfn=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  compute_log_prob(self, coords)\n",
      " |      Calculate the vector of log-probability for the walkers\n",
      " |\n",
      " |      Args:\n",
      " |          coords: (ndarray[..., ndim]) The position vector in parameter\n",
      " |              space where the probability should be calculated.\n",
      " |\n",
      " |      This method returns:\n",
      " |\n",
      " |      * log_prob: A vector of log-probabilities with one entry for each\n",
      " |        walker in this sub-ensemble.\n",
      " |      * blob: The list of meta data returned by the ``log_post_fn`` at\n",
      " |        this position or ``None`` if nothing was returned.\n",
      " |\n",
      " |  get_autocorr_time(self, **kwargs)\n",
      " |      Compute an estimate of the autocorrelation time for each parameter\n",
      " |\n",
      " |      Args:\n",
      " |          thin (Optional[int]): Use only every ``thin`` steps from the\n",
      " |              chain. The returned estimate is multiplied by ``thin`` so the\n",
      " |              estimated time is in units of steps, not thinned steps.\n",
      " |              (default: ``1``)\n",
      " |          discard (Optional[int]): Discard the first ``discard`` steps in\n",
      " |              the chain as burn-in. (default: ``0``)\n",
      " |\n",
      " |      Other arguments are passed directly to\n",
      " |      :func:`emcee.autocorr.integrated_time`.\n",
      " |\n",
      " |      Returns:\n",
      " |          array[ndim]: The integrated autocorrelation time estimate for the\n",
      " |              chain for each parameter.\n",
      " |\n",
      " |  get_blobs(self, **kwargs)\n",
      " |      Get the chain of blobs for each sample in the chain\n",
      " |\n",
      " |      Args:\n",
      " |          flat (Optional[bool]): Flatten the chain across the ensemble.\n",
      " |              (default: ``False``)\n",
      " |          thin (Optional[int]): Take only every ``thin`` steps from the\n",
      " |              chain. (default: ``1``)\n",
      " |          discard (Optional[int]): Discard the first ``discard`` steps in\n",
      " |              the chain as burn-in. (default: ``0``)\n",
      " |\n",
      " |      Returns:\n",
      " |          array[..., nwalkers]: The chain of blobs.\n",
      " |\n",
      " |  get_chain(self, **kwargs)\n",
      " |      Get the stored chain of MCMC samples\n",
      " |\n",
      " |      Args:\n",
      " |          flat (Optional[bool]): Flatten the chain across the ensemble.\n",
      " |              (default: ``False``)\n",
      " |          thin (Optional[int]): Take only every ``thin`` steps from the\n",
      " |              chain. (default: ``1``)\n",
      " |          discard (Optional[int]): Discard the first ``discard`` steps in\n",
      " |              the chain as burn-in. (default: ``0``)\n",
      " |\n",
      " |      Returns:\n",
      " |          array[..., nwalkers, ndim]: The MCMC samples.\n",
      " |\n",
      " |  get_last_sample(self, **kwargs)\n",
      " |      Access the most recent sample in the chain\n",
      " |\n",
      " |  get_log_prob(self, **kwargs)\n",
      " |      Get the chain of log probabilities evaluated at the MCMC samples\n",
      " |\n",
      " |      Args:\n",
      " |          flat (Optional[bool]): Flatten the chain across the ensemble.\n",
      " |              (default: ``False``)\n",
      " |          thin (Optional[int]): Take only every ``thin`` steps from the\n",
      " |              chain. (default: ``1``)\n",
      " |          discard (Optional[int]): Discard the first ``discard`` steps in\n",
      " |              the chain as burn-in. (default: ``0``)\n",
      " |\n",
      " |      Returns:\n",
      " |          array[..., nwalkers]: The chain of log probabilities.\n",
      " |\n",
      " |  get_value(self, name, **kwargs)\n",
      " |\n",
      " |  reset(self)\n",
      " |      Reset the bookkeeping parameters\n",
      " |\n",
      " |  run_mcmc(self, initial_state, nsteps, **kwargs)\n",
      " |      Iterate :func:`sample` for ``nsteps`` iterations and return the result\n",
      " |\n",
      " |      Args:\n",
      " |          initial_state: The initial state or position vector. Can also be\n",
      " |              ``None`` to resume from where :func:``run_mcmc`` left off the\n",
      " |              last time it executed.\n",
      " |          nsteps: The number of steps to run.\n",
      " |\n",
      " |      Other parameters are directly passed to :func:`sample`.\n",
      " |\n",
      " |      This method returns the most recent result from :func:`sample`.\n",
      " |\n",
      " |  sample(self, initial_state, log_prob0=None, rstate0=None, blobs0=None, iterations=1, tune=False, skip_initial_state_check=False, thin_by=1, thin=None, store=True, progress=False, progress_kwargs=None)\n",
      " |      Advance the chain as a generator\n",
      " |\n",
      " |      Args:\n",
      " |          initial_state (State or ndarray[nwalkers, ndim]): The initial\n",
      " |              :class:`State` or positions of the walkers in the\n",
      " |              parameter space.\n",
      " |          iterations (Optional[int or NoneType]): The number of steps to generate.\n",
      " |              ``None`` generates an infinite stream (requires ``store=False``).\n",
      " |          tune (Optional[bool]): If ``True``, the parameters of some moves\n",
      " |              will be automatically tuned.\n",
      " |          thin_by (Optional[int]): If you only want to store and yield every\n",
      " |              ``thin_by`` samples in the chain, set ``thin_by`` to an\n",
      " |              integer greater than 1. When this is set, ``iterations *\n",
      " |              thin_by`` proposals will be made.\n",
      " |          store (Optional[bool]): By default, the sampler stores (in memory)\n",
      " |              the positions and log-probabilities of the samples in the\n",
      " |              chain. If you are using another method to store the samples to\n",
      " |              a file or if you don't need to analyze the samples after the\n",
      " |              fact (for burn-in for example) set ``store`` to ``False``.\n",
      " |          progress (Optional[bool or str]): If ``True``, a progress bar will\n",
      " |              be shown as the sampler progresses. If a string, will select a\n",
      " |              specific ``tqdm`` progress bar - most notable is\n",
      " |              ``'notebook'``, which shows a progress bar suitable for\n",
      " |              Jupyter notebooks.  If ``False``, no progress bar will be\n",
      " |              shown.\n",
      " |          progress_kwargs (Optional[dict]): A ``dict`` of keyword arguments\n",
      " |              to be passed to the tqdm call.\n",
      " |          skip_initial_state_check (Optional[bool]): If ``True``, a check\n",
      " |              that the initial_state can fully explore the space will be\n",
      " |              skipped. (default: ``False``)\n",
      " |\n",
      " |\n",
      " |      Every ``thin_by`` steps, this generator yields the\n",
      " |      :class:`State` of the ensemble.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  acceptance_fraction\n",
      " |      The fraction of proposed steps that were accepted\n",
      " |\n",
      " |  acor\n",
      " |\n",
      " |  blobs\n",
      " |\n",
      " |  chain\n",
      " |\n",
      " |  flatblobs\n",
      " |\n",
      " |  flatchain\n",
      " |\n",
      " |  flatlnprobability\n",
      " |\n",
      " |  iteration\n",
      " |\n",
      " |  lnprobability\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  random_state\n",
      " |      The state of the internal random number generator. In practice, it's\n",
      " |      the result of calling ``get_state()`` on a\n",
      " |      ``numpy.random.mtrand.RandomState`` object. You can try to set this\n",
      " |      property but be warned that if you do this and it fails, it will do\n",
      " |      so silently.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(emcee.EnsembleSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f24409a-e325-4103-a412-71aafd71eb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47628129, 0.96412469, 0.24411467, 0.5481956 , 0.32095011,\n",
       "       0.52541791, 0.63221125, 0.48021056, 0.46275819, 0.73930419])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8f098fc9-0168-4d12-966a-7aeb8cfce4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.19202922e-04,  3.20586033e+07],\n",
       "       [-0.00000000e+00,  1.19202922e+08],\n",
       "       [ 0.00000000e+00,  3.56085740e+08],\n",
       "       [-3.20586033e-05, -1.19202922e+08],\n",
       "       [-3.56085740e-04,  0.00000000e+00],\n",
       "       [ 1.00000000e-03, -3.20586033e+07],\n",
       "       [-1.00000000e-03, -3.56085740e+08],\n",
       "       [ 3.20586033e-05, -1.00000000e+09],\n",
       "       [ 3.56085740e-04, -0.00000000e+00],\n",
       "       [ 1.19202922e-04,  1.00000000e+09]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _sample_gaussian(prior_mean,\n",
    "                     prior_std,\n",
    "                     lower_bound,\n",
    "                     upper_bound,\n",
    "                     num_walkers):\n",
    "    \n",
    "    # generate a huge number of possible priors \n",
    "    gaussian_priors = np.random.normal(loc=prior_mean,\n",
    "                                       scale=prior_std,\n",
    "                                       size=num_walkers*1000)\n",
    "    \n",
    "    # Grab only those priors that are within the bounds\n",
    "    good_mask = np.logical_and(gaussian_priors > lower_bound,\n",
    "                               gaussian_priors < upper_bound)\n",
    "    good_priors = gaussian_priors[good_mask]\n",
    "    \n",
    "    # If we have enough good priors, keep them. If we have only a few\n",
    "    # good priors, it means the bounds have sliced out a ridiculously\n",
    "    # tiny chunk of the distribution. Approximate the walkers as \n",
    "    # a uniform sample from that distribution. \n",
    "    if len(good_priors) >= num_walkers:\n",
    "        return good_priors[:num_walkers]\n",
    "\n",
    "    return None\n",
    "\n",
    "def _sample_uniform(lower_bound,\n",
    "                    upper_bound,\n",
    "                    num_walkers,\n",
    "                    infinity_proxy):\n",
    "\n",
    "    # Slice down infinite bounds to largish numbers\n",
    "    if np.isinf(lower_bound): \n",
    "        lower_bound = -infinity_proxy\n",
    "    if np.isinf(upper_bound):\n",
    "        upper_bound = infinity_proxy\n",
    "\n",
    "    # If only one walker, put at the mean of the bounds\n",
    "    if num_walkers == 1:\n",
    "        return [np.mean([lower_bound,upper_bound])]\n",
    "\n",
    "    # If the upper and lower bounds have the same sign, make a uniform\n",
    "    # span between them (log steps). For example, 1e-9 to 1e-6 with four\n",
    "    # walkers would yield 1e-9, 1e-8, 1e-7, 1e-6\n",
    "    if upper_bound*lower_bound > 0:\n",
    "        \n",
    "        steps = np.exp(np.arange(num_walkers))\n",
    "        steps = (steps - np.min(steps))/(np.max(steps) - np.min(steps))\n",
    "        walkers = steps*(upper_bound - lower_bound) + lower_bound\n",
    "        np.random.shuffle(walkers)\n",
    "        \n",
    "        return walkers\n",
    "\n",
    "    # If the upper and lower bounds have different signs, make uniform\n",
    "    # spans from 0 to upper and 0 to lower, weighted by how much of the\n",
    "    # the span is above and below. \n",
    "    \n",
    "    # Figure out fraction of uniform distribution below zero\n",
    "    lower_mag = np.abs(lower_bound)\n",
    "    upper_mag = np.abs(upper_bound)\n",
    "    fx_lower = lower_mag/(lower_mag + upper_mag)\n",
    "\n",
    "    # Figure out how many walkers to place above and below zero\n",
    "    num_below = int(np.round(fx_lower*num_walkers,0))\n",
    "\n",
    "    # Make sure we have at least one above and one below\n",
    "    if num_below == 0: \n",
    "        num_below = 1\n",
    "    if num_below == num_walkers:\n",
    "        num_below = num_walkers - 1\n",
    "    num_above = num_walkers - num_below\n",
    "\n",
    "    # Create steps from 0 to upper_bound\n",
    "    steps = np.exp(np.arange(num_above))\n",
    "    steps = (steps - np.min(steps))/(np.max(steps) - np.min(steps))\n",
    "    above_walkers = list(steps*upper_bound)\n",
    "\n",
    "    # Create steps from 0 to lower_bound\n",
    "    steps = np.exp(np.arange(num_below))\n",
    "    steps = (steps - np.min(steps))/(np.max(steps) - np.min(steps))\n",
    "    below_walkers = list(steps*lower_bound)\n",
    "\n",
    "    # Combine all steps\n",
    "    above_walkers.extend(below_walkers)\n",
    "    walkers = np.array(above_walkers)\n",
    "\n",
    "    # Shuffle randomly\n",
    "    np.random.shuffle(walkers)\n",
    "\n",
    "    return walkers\n",
    "\n",
    "\n",
    "def _create_walkers(param_df,\n",
    "                    num_walkers,\n",
    "                    infinity_proxy=1e9):\n",
    "\n",
    "    walker_list = []\n",
    "\n",
    "    # Go through each parameter one-by-one\n",
    "    for p in param_df.index:\n",
    "        \n",
    "        # Skip fixed parameters\n",
    "        if param_df.loc[p,\"fixed\"]:\n",
    "            continue\n",
    "\n",
    "        # Get prior mean, std, and bounds\n",
    "        guess = param_df.loc[p,\"guess\"]\n",
    "        prior_mean = param_df.loc[p,\"prior_mean\"]\n",
    "        prior_std = param_df.loc[p,\"prior_std\"]\n",
    "        lower_bound = param_df.loc[p,\"lower_bound\"]\n",
    "        upper_bound = param_df.loc[p,\"upper_bound\"]\n",
    "\n",
    "        # If gaussian prior, try to do that first. \n",
    "        if not np.isnan(prior_mean):\n",
    "\n",
    "            gaussian_priors = _sample_gaussian(prior_mean,\n",
    "                                               prior_std,\n",
    "                                               lower_bound,\n",
    "                                               upper_bound,\n",
    "                                               num_walkers)\n",
    "            if gaussian_priors is not None:\n",
    "                walker_list.append(gaussian_priors)\n",
    "                continue\n",
    "\n",
    "        # If we get here, gaussian priors were not given or did not work.\n",
    "        uniform_priors = _sample_uniform(lower_bound,\n",
    "                                         upper_bound,\n",
    "                                         num_walkers,\n",
    "                                         infinity_proxy)\n",
    "        walker_list.append(uniform_priors)\n",
    "        \n",
    "    walkers = np.array(walker_list).T\n",
    "\n",
    "    return walkers\n",
    "\n",
    "\n",
    "param_df = pd.DataFrame({\"name\":[\"a\",\"b\"],\n",
    "                        \"guess\":[1,2],\n",
    "                        \"fixed\":[False,False],\n",
    "                        \"prior_mean\":[np.nan,np.nan],\n",
    "                        \"prior_std\":[2,np.nan],\n",
    "                        \"lower_bound\":[-0.001,-np.inf],\n",
    "                        \"upper_bound\":[0.001,np.inf]})\n",
    "\n",
    "s = _create_walkers(param_df=param_df,\n",
    "                num_walkers=10)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7aae1500-52fb-4f4f-bfdc-f929531c3375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.907755278982137"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)\n",
    "np.log(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37b6db98-9787-47a1-9aee-767514221b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_dist = stats.sampling.FastGeneratorInversion(rv)\n",
    "x = prior_dist.rvs((10,2))\n",
    "means = np.array([0,1])\n",
    "stds = np.array([10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4cbfd5fc-c9a1-4847-8969-767319df0198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00309253, -0.65680484],\n",
       "       [-0.10013265, -2.06772866],\n",
       "       [-0.01153172, -0.97799837],\n",
       "       [ 0.29657577, -0.38081001],\n",
       "       [-0.06001777,  0.47087776],\n",
       "       [-0.03209133, -2.95918773],\n",
       "       [ 0.06411045, -1.43260952],\n",
       "       [-0.09383089, -0.040393  ],\n",
       "       [-0.03906135, -1.07416001],\n",
       "       [ 0.06609788, -1.31870454]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x - means)/stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d861d704-3374-4fcd-8aba-220dbf3e019a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
