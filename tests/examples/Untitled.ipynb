{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "816f33e1-6223-4ba5-b862-f1ded5009767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import emcee\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed142691-9073-48fe-b3be-0c158fc4d5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_prob(param): return -param[0]*param[1]\n",
    "num_steps = 10\n",
    "num_walkers = 10\n",
    "ndim = 2\n",
    "pos = [np.random.normal(loc=0,scale=1,size=2)\n",
    "       for _ in range(num_walkers)]\n",
    "\n",
    "es = emcee.EnsembleSampler(num_walkers,\n",
    "                           ndim,\n",
    "                           ln_prob)\n",
    "yo = es.run_mcmc(pos,num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fec6475a-7dcf-4da9-8bb9-08ed012f08c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EnsembleSampler in module emcee.ensemble:\n",
      "\n",
      "class EnsembleSampler(builtins.object)\n",
      " |  EnsembleSampler(nwalkers, ndim, log_prob_fn, pool=None, moves=None, args=None, kwargs=None, backend=None, vectorize=False, blobs_dtype=None, parameter_names: Union[Dict[str, int], List[str], NoneType] = None, a=None, postargs=None, threads=None, live_dangerously=None, runtime_sortingfn=None)\n",
      " |\n",
      " |  An ensemble MCMC sampler\n",
      " |\n",
      " |  If you are upgrading from an earlier version of emcee, you might notice\n",
      " |  that some arguments are now deprecated. The parameters that control the\n",
      " |  proposals have been moved to the :ref:`moves-user` interface (``a`` and\n",
      " |  ``live_dangerously``), and the parameters related to parallelization can\n",
      " |  now be controlled via the ``pool`` argument (:ref:`parallel`).\n",
      " |\n",
      " |  Args:\n",
      " |      nwalkers (int): The number of walkers in the ensemble.\n",
      " |      ndim (int): Number of dimensions in the parameter space.\n",
      " |      log_prob_fn (callable): A function that takes a vector in the\n",
      " |          parameter space as input and returns the natural logarithm of the\n",
      " |          posterior probability (up to an additive constant) for that\n",
      " |          position.\n",
      " |      moves (Optional): This can be a single move object, a list of moves,\n",
      " |          or a \"weighted\" list of the form ``[(emcee.moves.StretchMove(),\n",
      " |          0.1), ...]``. When running, the sampler will randomly select a\n",
      " |          move from this list (optionally with weights) for each proposal.\n",
      " |          (default: :class:`StretchMove`)\n",
      " |      args (Optional): A list of extra positional arguments for\n",
      " |          ``log_prob_fn``. ``log_prob_fn`` will be called with the sequence\n",
      " |          ``log_prob_fn(p, *args, **kwargs)``.\n",
      " |      kwargs (Optional): A dict of extra keyword arguments for\n",
      " |          ``log_prob_fn``. ``log_prob_fn`` will be called with the sequence\n",
      " |          ``log_prob_fn(p, *args, **kwargs)``.\n",
      " |      pool (Optional): An object with a ``map`` method that follows the same\n",
      " |          calling sequence as the built-in ``map`` function. This is\n",
      " |          generally used to compute the log-probabilities for the ensemble\n",
      " |          in parallel.\n",
      " |      backend (Optional): Either a :class:`backends.Backend` or a subclass\n",
      " |          (like :class:`backends.HDFBackend`) that is used to store and\n",
      " |          serialize the state of the chain. By default, the chain is stored\n",
      " |          as a set of numpy arrays in memory, but new backends can be\n",
      " |          written to support other mediums.\n",
      " |      vectorize (Optional[bool]): If ``True``, ``log_prob_fn`` is expected\n",
      " |          to accept a list of position vectors instead of just one. Note\n",
      " |          that ``pool`` will be ignored if this is ``True``.\n",
      " |          (default: ``False``)\n",
      " |      parameter_names (Optional[Union[List[str], Dict[str, List[int]]]]):\n",
      " |          names of individual parameters or groups of parameters. If\n",
      " |          specified, the ``log_prob_fn`` will recieve a dictionary of\n",
      " |          parameters, rather than a ``np.ndarray``.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __init__(self, nwalkers, ndim, log_prob_fn, pool=None, moves=None, args=None, kwargs=None, backend=None, vectorize=False, blobs_dtype=None, parameter_names: Union[Dict[str, int], List[str], NoneType] = None, a=None, postargs=None, threads=None, live_dangerously=None, runtime_sortingfn=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  compute_log_prob(self, coords)\n",
      " |      Calculate the vector of log-probability for the walkers\n",
      " |\n",
      " |      Args:\n",
      " |          coords: (ndarray[..., ndim]) The position vector in parameter\n",
      " |              space where the probability should be calculated.\n",
      " |\n",
      " |      This method returns:\n",
      " |\n",
      " |      * log_prob: A vector of log-probabilities with one entry for each\n",
      " |        walker in this sub-ensemble.\n",
      " |      * blob: The list of meta data returned by the ``log_post_fn`` at\n",
      " |        this position or ``None`` if nothing was returned.\n",
      " |\n",
      " |  get_autocorr_time(self, **kwargs)\n",
      " |      Compute an estimate of the autocorrelation time for each parameter\n",
      " |\n",
      " |      Args:\n",
      " |          thin (Optional[int]): Use only every ``thin`` steps from the\n",
      " |              chain. The returned estimate is multiplied by ``thin`` so the\n",
      " |              estimated time is in units of steps, not thinned steps.\n",
      " |              (default: ``1``)\n",
      " |          discard (Optional[int]): Discard the first ``discard`` steps in\n",
      " |              the chain as burn-in. (default: ``0``)\n",
      " |\n",
      " |      Other arguments are passed directly to\n",
      " |      :func:`emcee.autocorr.integrated_time`.\n",
      " |\n",
      " |      Returns:\n",
      " |          array[ndim]: The integrated autocorrelation time estimate for the\n",
      " |              chain for each parameter.\n",
      " |\n",
      " |  get_blobs(self, **kwargs)\n",
      " |      Get the chain of blobs for each sample in the chain\n",
      " |\n",
      " |      Args:\n",
      " |          flat (Optional[bool]): Flatten the chain across the ensemble.\n",
      " |              (default: ``False``)\n",
      " |          thin (Optional[int]): Take only every ``thin`` steps from the\n",
      " |              chain. (default: ``1``)\n",
      " |          discard (Optional[int]): Discard the first ``discard`` steps in\n",
      " |              the chain as burn-in. (default: ``0``)\n",
      " |\n",
      " |      Returns:\n",
      " |          array[..., nwalkers]: The chain of blobs.\n",
      " |\n",
      " |  get_chain(self, **kwargs)\n",
      " |      Get the stored chain of MCMC samples\n",
      " |\n",
      " |      Args:\n",
      " |          flat (Optional[bool]): Flatten the chain across the ensemble.\n",
      " |              (default: ``False``)\n",
      " |          thin (Optional[int]): Take only every ``thin`` steps from the\n",
      " |              chain. (default: ``1``)\n",
      " |          discard (Optional[int]): Discard the first ``discard`` steps in\n",
      " |              the chain as burn-in. (default: ``0``)\n",
      " |\n",
      " |      Returns:\n",
      " |          array[..., nwalkers, ndim]: The MCMC samples.\n",
      " |\n",
      " |  get_last_sample(self, **kwargs)\n",
      " |      Access the most recent sample in the chain\n",
      " |\n",
      " |  get_log_prob(self, **kwargs)\n",
      " |      Get the chain of log probabilities evaluated at the MCMC samples\n",
      " |\n",
      " |      Args:\n",
      " |          flat (Optional[bool]): Flatten the chain across the ensemble.\n",
      " |              (default: ``False``)\n",
      " |          thin (Optional[int]): Take only every ``thin`` steps from the\n",
      " |              chain. (default: ``1``)\n",
      " |          discard (Optional[int]): Discard the first ``discard`` steps in\n",
      " |              the chain as burn-in. (default: ``0``)\n",
      " |\n",
      " |      Returns:\n",
      " |          array[..., nwalkers]: The chain of log probabilities.\n",
      " |\n",
      " |  get_value(self, name, **kwargs)\n",
      " |\n",
      " |  reset(self)\n",
      " |      Reset the bookkeeping parameters\n",
      " |\n",
      " |  run_mcmc(self, initial_state, nsteps, **kwargs)\n",
      " |      Iterate :func:`sample` for ``nsteps`` iterations and return the result\n",
      " |\n",
      " |      Args:\n",
      " |          initial_state: The initial state or position vector. Can also be\n",
      " |              ``None`` to resume from where :func:``run_mcmc`` left off the\n",
      " |              last time it executed.\n",
      " |          nsteps: The number of steps to run.\n",
      " |\n",
      " |      Other parameters are directly passed to :func:`sample`.\n",
      " |\n",
      " |      This method returns the most recent result from :func:`sample`.\n",
      " |\n",
      " |  sample(self, initial_state, log_prob0=None, rstate0=None, blobs0=None, iterations=1, tune=False, skip_initial_state_check=False, thin_by=1, thin=None, store=True, progress=False, progress_kwargs=None)\n",
      " |      Advance the chain as a generator\n",
      " |\n",
      " |      Args:\n",
      " |          initial_state (State or ndarray[nwalkers, ndim]): The initial\n",
      " |              :class:`State` or positions of the walkers in the\n",
      " |              parameter space.\n",
      " |          iterations (Optional[int or NoneType]): The number of steps to generate.\n",
      " |              ``None`` generates an infinite stream (requires ``store=False``).\n",
      " |          tune (Optional[bool]): If ``True``, the parameters of some moves\n",
      " |              will be automatically tuned.\n",
      " |          thin_by (Optional[int]): If you only want to store and yield every\n",
      " |              ``thin_by`` samples in the chain, set ``thin_by`` to an\n",
      " |              integer greater than 1. When this is set, ``iterations *\n",
      " |              thin_by`` proposals will be made.\n",
      " |          store (Optional[bool]): By default, the sampler stores (in memory)\n",
      " |              the positions and log-probabilities of the samples in the\n",
      " |              chain. If you are using another method to store the samples to\n",
      " |              a file or if you don't need to analyze the samples after the\n",
      " |              fact (for burn-in for example) set ``store`` to ``False``.\n",
      " |          progress (Optional[bool or str]): If ``True``, a progress bar will\n",
      " |              be shown as the sampler progresses. If a string, will select a\n",
      " |              specific ``tqdm`` progress bar - most notable is\n",
      " |              ``'notebook'``, which shows a progress bar suitable for\n",
      " |              Jupyter notebooks.  If ``False``, no progress bar will be\n",
      " |              shown.\n",
      " |          progress_kwargs (Optional[dict]): A ``dict`` of keyword arguments\n",
      " |              to be passed to the tqdm call.\n",
      " |          skip_initial_state_check (Optional[bool]): If ``True``, a check\n",
      " |              that the initial_state can fully explore the space will be\n",
      " |              skipped. (default: ``False``)\n",
      " |\n",
      " |\n",
      " |      Every ``thin_by`` steps, this generator yields the\n",
      " |      :class:`State` of the ensemble.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  acceptance_fraction\n",
      " |      The fraction of proposed steps that were accepted\n",
      " |\n",
      " |  acor\n",
      " |\n",
      " |  blobs\n",
      " |\n",
      " |  chain\n",
      " |\n",
      " |  flatblobs\n",
      " |\n",
      " |  flatchain\n",
      " |\n",
      " |  flatlnprobability\n",
      " |\n",
      " |  iteration\n",
      " |\n",
      " |  lnprobability\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  random_state\n",
      " |      The state of the internal random number generator. In practice, it's\n",
      " |      the result of calling ``get_state()`` on a\n",
      " |      ``numpy.random.mtrand.RandomState`` object. You can try to set this\n",
      " |      property but be warned that if you do this and it fails, it will do\n",
      " |      so silently.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(emcee.EnsembleSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f24409a-e325-4103-a412-71aafd71eb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47628129, 0.96412469, 0.24411467, 0.5481956 , 0.32095011,\n",
       "       0.52541791, 0.63221125, 0.48021056, 0.46275819, 0.73930419])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8f098fc9-0168-4d12-966a-7aeb8cfce4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.19202922e-04,  3.20586033e+07],\n",
       "       [-0.00000000e+00,  1.19202922e+08],\n",
       "       [ 0.00000000e+00,  3.56085740e+08],\n",
       "       [-3.20586033e-05, -1.19202922e+08],\n",
       "       [-3.56085740e-04,  0.00000000e+00],\n",
       "       [ 1.00000000e-03, -3.20586033e+07],\n",
       "       [-1.00000000e-03, -3.56085740e+08],\n",
       "       [ 3.20586033e-05, -1.00000000e+09],\n",
       "       [ 3.56085740e-04, -0.00000000e+00],\n",
       "       [ 1.19202922e-04,  1.00000000e+09]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _sample_gaussian(prior_mean,\n",
    "                     prior_std,\n",
    "                     lower_bound,\n",
    "                     upper_bound,\n",
    "                     num_walkers):\n",
    "    \n",
    "    # generate a huge number of possible priors \n",
    "    gaussian_priors = np.random.normal(loc=prior_mean,\n",
    "                                       scale=prior_std,\n",
    "                                       size=num_walkers*1000)\n",
    "    \n",
    "    # Grab only those priors that are within the bounds\n",
    "    good_mask = np.logical_and(gaussian_priors > lower_bound,\n",
    "                               gaussian_priors < upper_bound)\n",
    "    good_priors = gaussian_priors[good_mask]\n",
    "    \n",
    "    # If we have enough good priors, keep them. If we have only a few\n",
    "    # good priors, it means the bounds have sliced out a ridiculously\n",
    "    # tiny chunk of the distribution. Approximate the walkers as \n",
    "    # a uniform sample from that distribution. \n",
    "    if len(good_priors) >= num_walkers:\n",
    "        return good_priors[:num_walkers]\n",
    "\n",
    "    return None\n",
    "\n",
    "def _sample_uniform(lower_bound,\n",
    "                    upper_bound,\n",
    "                    num_walkers,\n",
    "                    infinity_proxy):\n",
    "\n",
    "    # Slice down infinite bounds to largish numbers\n",
    "    if np.isinf(lower_bound): \n",
    "        lower_bound = -infinity_proxy\n",
    "    if np.isinf(upper_bound):\n",
    "        upper_bound = infinity_proxy\n",
    "\n",
    "    # If only one walker, put at the mean of the bounds\n",
    "    if num_walkers == 1:\n",
    "        return [np.mean([lower_bound,upper_bound])]\n",
    "\n",
    "    # If the upper and lower bounds have the same sign, make a uniform\n",
    "    # span between them (log steps). For example, 1e-9 to 1e-6 with four\n",
    "    # walkers would yield 1e-9, 1e-8, 1e-7, 1e-6\n",
    "    if upper_bound*lower_bound > 0:\n",
    "        \n",
    "        steps = np.exp(np.arange(num_walkers))\n",
    "        steps = (steps - np.min(steps))/(np.max(steps) - np.min(steps))\n",
    "        walkers = steps*(upper_bound - lower_bound) + lower_bound\n",
    "        np.random.shuffle(walkers)\n",
    "        \n",
    "        return walkers\n",
    "\n",
    "    # If the upper and lower bounds have different signs, make uniform\n",
    "    # spans from 0 to upper and 0 to lower, weighted by how much of the\n",
    "    # the span is above and below. \n",
    "    \n",
    "    # Figure out fraction of uniform distribution below zero\n",
    "    lower_mag = np.abs(lower_bound)\n",
    "    upper_mag = np.abs(upper_bound)\n",
    "    fx_lower = lower_mag/(lower_mag + upper_mag)\n",
    "\n",
    "    # Figure out how many walkers to place above and below zero\n",
    "    num_below = int(np.round(fx_lower*num_walkers,0))\n",
    "\n",
    "    # Make sure we have at least one above and one below\n",
    "    if num_below == 0: \n",
    "        num_below = 1\n",
    "    if num_below == num_walkers:\n",
    "        num_below = num_walkers - 1\n",
    "    num_above = num_walkers - num_below\n",
    "\n",
    "    # Create steps from 0 to upper_bound\n",
    "    steps = np.exp(np.arange(num_above))\n",
    "    steps = (steps - np.min(steps))/(np.max(steps) - np.min(steps))\n",
    "    above_walkers = list(steps*upper_bound)\n",
    "\n",
    "    # Create steps from 0 to lower_bound\n",
    "    steps = np.exp(np.arange(num_below))\n",
    "    steps = (steps - np.min(steps))/(np.max(steps) - np.min(steps))\n",
    "    below_walkers = list(steps*lower_bound)\n",
    "\n",
    "    # Combine all steps\n",
    "    above_walkers.extend(below_walkers)\n",
    "    walkers = np.array(above_walkers)\n",
    "\n",
    "    # Shuffle randomly\n",
    "    np.random.shuffle(walkers)\n",
    "\n",
    "    return walkers\n",
    "\n",
    "\n",
    "def _create_walkers(param_df,\n",
    "                    num_walkers,\n",
    "                    infinity_proxy=1e9):\n",
    "\n",
    "    walker_list = []\n",
    "\n",
    "    # Go through each parameter one-by-one\n",
    "    for p in param_df.index:\n",
    "        \n",
    "        # Skip fixed parameters\n",
    "        if param_df.loc[p,\"fixed\"]:\n",
    "            continue\n",
    "\n",
    "        # Get prior mean, std, and bounds\n",
    "        guess = param_df.loc[p,\"guess\"]\n",
    "        prior_mean = param_df.loc[p,\"prior_mean\"]\n",
    "        prior_std = param_df.loc[p,\"prior_std\"]\n",
    "        lower_bound = param_df.loc[p,\"lower_bound\"]\n",
    "        upper_bound = param_df.loc[p,\"upper_bound\"]\n",
    "\n",
    "        # If gaussian prior, try to do that first. \n",
    "        if not np.isnan(prior_mean):\n",
    "\n",
    "            gaussian_priors = _sample_gaussian(prior_mean,\n",
    "                                               prior_std,\n",
    "                                               lower_bound,\n",
    "                                               upper_bound,\n",
    "                                               num_walkers)\n",
    "            if gaussian_priors is not None:\n",
    "                walker_list.append(gaussian_priors)\n",
    "                continue\n",
    "\n",
    "        # If we get here, gaussian priors were not given or did not work.\n",
    "        uniform_priors = _sample_uniform(lower_bound,\n",
    "                                         upper_bound,\n",
    "                                         num_walkers,\n",
    "                                         infinity_proxy)\n",
    "        walker_list.append(uniform_priors)\n",
    "        \n",
    "    walkers = np.array(walker_list).T\n",
    "\n",
    "    return walkers\n",
    "\n",
    "\n",
    "param_df = pd.DataFrame({\"name\":[\"a\",\"b\"],\n",
    "                        \"guess\":[1,2],\n",
    "                        \"fixed\":[False,False],\n",
    "                        \"prior_mean\":[np.nan,np.nan],\n",
    "                        \"prior_std\":[2,np.nan],\n",
    "                        \"lower_bound\":[-0.001,-np.inf],\n",
    "                        \"upper_bound\":[0.001,np.inf]})\n",
    "\n",
    "s = _create_walkers(param_df=param_df,\n",
    "                num_walkers=10)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7aae1500-52fb-4f4f-bfdc-f929531c3375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.907755278982137"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)\n",
    "np.log(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37b6db98-9787-47a1-9aee-767514221b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_dist = stats.sampling.FastGeneratorInversion(rv)\n",
    "x = prior_dist.rvs((10,2))\n",
    "means = np.array([0,1])\n",
    "stds = np.array([10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4cbfd5fc-c9a1-4847-8969-767319df0198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00309253, -0.65680484],\n",
       "       [-0.10013265, -2.06772866],\n",
       "       [-0.01153172, -0.97799837],\n",
       "       [ 0.29657577, -0.38081001],\n",
       "       [-0.06001777,  0.47087776],\n",
       "       [-0.03209133, -2.95918773],\n",
       "       [ 0.06411045, -1.43260952],\n",
       "       [-0.09383089, -0.040393  ],\n",
       "       [-0.03906135, -1.07416001],\n",
       "       [ 0.06609788, -1.31870454]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x - means)/stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d861d704-3374-4fcd-8aba-220dbf3e019a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
